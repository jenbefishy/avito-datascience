{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5268dcf2",
   "metadata": {},
   "source": [
    "# Алгоритм \n",
    "\n",
    "**Идея**: динамическое программирование по униграммной языковой модели + пост-обработка.  \n",
    "\n",
    "1. **Словари частот**: загружаем RU/EN списки слов.  \n",
    "   - logP(w) = log(freq/total).  \n",
    "   - Спец-эвристики: бренды/модели, числа, пунктуация, дефисы/тире\n",
    "\n",
    "\n",
    "\n",
    "3. **DP**: для каждой позиции *i* перебираем кандидатов j<i  \n",
    "   выбираем разрез с максимальной суммой logP.  \n",
    "\n",
    "4. **Пост-обработка**:  \n",
    "   - Скобки `()[]{}`, «ёлочки», кавычки `\"` и `'` (открывающие/закрывающие, апострофы внутри слов).  \n",
    "   - Пунктуация `,.;:!?…` — убираем пробелы *перед*, ставим *после* при необходимости.  \n",
    "   - Склеиваем `...` и юникодное `…`.  \n",
    "   - Русские клитики: `-то` (без пробелов вокруг дефиса).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b727be71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05c2517",
   "metadata": {},
   "source": [
    "## Глобальные настройки и словари\n",
    "* Расширяем набор пунктуации (учитываем тире/многоточие/«ёлочки»).\n",
    "* Определяем список «особых» терминов (бренды/модели и т.д.), чтобы не штрафовать их разбиение.\n",
    "* Собираем special_mapping: конкатенированная форма → «нормальная». Это даёт большой приоритет распознаванию спец-слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92a2c9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "punct = string.punctuation + '—–−…«»'\n",
    "attaching_punct = set(punct) - {'-', '—', '–', '−'}\n",
    "\n",
    "special_terms = [\n",
    "    'USSR', 'ЕГЭ', 'iPhone', 'HUAWEI', 'Huawei', 'Playstation', 'PLAYSTATION',\n",
    "    'Philips', 'HP', 'Samsung', 'Fender', 'Xbox One', 'Xiaomi', 'Indesit',\n",
    "    'Merida', 'LG', 'Yamaha', 'Ikea', 'Asus', 'Dyson', 'Stels', 'Oppo', 'Honor',\n",
    "    'Bosch', 'Adidas', 'TP-Link', 'JBL', 'Levi’s', 'Canon', 'Pixel', 'MSI',\n",
    "    'Gibson', 'Atlant', 'TCL', 'Lenovo', 'Sharp', 'JVC', 'Acer', 'Dell', 'ROG',\n",
    "    'Toshiba', 'Mac', 'OLED', 'LG OLED', 'A54', 'XR', 'RTX', 'Avito',\n",
    "    'Яндекс Еда', 'Сбербанк', 'шкаф-купе', 'стиралок', 'вайфай', '5.1',\n",
    "    'Samsung A54', 'вайфай 6'\n",
    "]\n",
    "\n",
    "special_mapping = {}\n",
    "for t in special_terms:\n",
    "    low_t = t.lower()\n",
    "    concat = re.sub(r'[\\s-]', '', low_t)\n",
    "    special_mapping[concat] = t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d81daa",
   "metadata": {},
   "source": [
    "### Загрузка словарей частот (ru/en)\n",
    "* Скачиваем частотные списки слов (RU/EN) из открытого репозитория.\n",
    "* Строим словари частот word_freq_ru и word_freq_en и сохраняем агрегаты (total_freq_*, max_freq_*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63d125c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружаю словари...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_word_freq(lang='ru'):\n",
    "    if lang == 'ru':\n",
    "        url = \"https://raw.githubusercontent.com/hermitdave/FrequencyWords/master/content/2018/ru/ru_full.txt\"\n",
    "    elif lang == 'en':\n",
    "        url = \"https://raw.githubusercontent.com/hermitdave/FrequencyWords/master/content/2018/en/en_full.txt\"\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported language\")\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        lines = response.text.splitlines()\n",
    "        word_freq = {}\n",
    "        for line in lines:\n",
    "            parts = line.split(' ')\n",
    "            if len(parts) == 2:\n",
    "                word, freq = parts\n",
    "                word_freq[word.lower()] = int(freq)\n",
    "        return word_freq\n",
    "    else:\n",
    "        raise Exception(f\"Failed to download {lang} frequency list\")\n",
    "\n",
    "print(\"Загружаю словари...\")\n",
    "word_freq_ru = get_word_freq('ru')\n",
    "total_freq_ru = sum(word_freq_ru.values())\n",
    "max_freq_ru = max(word_freq_ru.values()) if word_freq_ru else 1\n",
    "\n",
    "word_freq_en = get_word_freq('en')\n",
    "total_freq_en = sum(word_freq_en.values())\n",
    "max_freq_en = max(word_freq_en.values()) if word_freq_en else 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9e482a",
   "metadata": {},
   "source": [
    "## Считаем вероятность слова\n",
    "* Если слово найдено в словаре — берём log(freq/total).\n",
    "* Для спец-терминов — считаем вероятность очень высокой.\n",
    "* Для цифр/пунктуации — отдельные бонусы (логика для 123, 5.1, дефисных форм)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13c0154",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_unigram_logp(word, word_freq, total_freq, max_freq):\n",
    "    alpha = 0.05\n",
    "    epsilon = 1e-9\n",
    "    if word in special_mapping:\n",
    "        return math.log(max_freq * 1e10 / total_freq)\n",
    "    if word in word_freq:\n",
    "        return math.log(word_freq[word] / total_freq)\n",
    "    elif word.isdigit():\n",
    "        return math.log(max_freq * 10 / total_freq)\n",
    "    elif len(word) > 0 and all(c in punct for c in word):\n",
    "        return math.log(max_freq * 50 / total_freq)\n",
    "    \n",
    "    for sep in ['-', '–', '—']:\n",
    "        if sep in word:\n",
    "            parts = [p for p in word.split(sep) if p]\n",
    "            if len(parts) > 1 and all(\n",
    "                p and (p in word_freq or len(p) > 1 or p.isdigit())\n",
    "                for p in parts\n",
    "            ):\n",
    "                sub_logps = [get_unigram_logp(p, word_freq, total_freq, max_freq)\n",
    "                             for p in parts]\n",
    "                sep_logp = math.log(max_freq * 50 / total_freq)\n",
    "                return sum(sub_logps) + (len(parts) - 1) * sep_logp + math.log(1.01)\n",
    "    \n",
    "    if '.' in word:\n",
    "        parts = [p for p in word.split('.') if p]\n",
    "        if len(parts) > 1 and all(p.isdigit() for p in parts):\n",
    "            sub_logps = [get_unigram_logp(p, word_freq, total_freq, max_freq)\n",
    "                         for p in parts]\n",
    "            sep_logp = math.log(max_freq * 50 / total_freq)\n",
    "            return sum(sub_logps) + (len(parts) - 1) * sep_logp + math.log(1.01)\n",
    "    \n",
    "    return math.log(epsilon) + (len(word) - 1) * math.log(alpha)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62be628",
   "metadata": {},
   "source": [
    "## Пост-обработка индексов\n",
    "Обрабатываем результат DP для корректной типографики.\n",
    "\n",
    "**Скобки и кавычки:**\n",
    "- Пробел перед открывающей, после неё убираем.\n",
    "- Перед закрывающей пробел убираем, после ставим.  \n",
    "  Пример: `\" Вперёд \"` -> `\"Вперёд\"`\n",
    "\n",
    "**Знаки пунктуации:**\n",
    "- Убираем пробелы перед `, . ; : ! ? …`.\n",
    "- Ставим пробел после при необходимости.\n",
    "\n",
    "**Дефис:**\n",
    "- Убираем пробелы вокруг дефиса в конструкциях с `-то`.  \n",
    "  Пример: `кто - то` -> `кто-то`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e02e94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPEN_BRACKETS  = set('([{«')\n",
    "CLOSE_BRACKETS = set(')]}»')\n",
    "TRAILING_PUNCT = set(',.;:!?')\n",
    "DOT = '.'\n",
    "ELLIPSIS = '…'\n",
    "SINGLE_QUOTE = \"'\"\n",
    "\n",
    "def fix_bracket_and_quote_spacing(orig_text, indices):\n",
    "    s = orig_text\n",
    "    n = len(s)\n",
    "    idx = {i for i in indices if 0 < i < n}\n",
    "\n",
    "    for pos, ch in enumerate(s):\n",
    "        if ch in OPEN_BRACKETS:\n",
    "            idx.discard(pos + 1)\n",
    "            if pos > 0 and (s[pos - 1].isalnum() or s[pos - 1] in CLOSE_BRACKETS or s[pos - 1] in {'\"', \"'\"}):\n",
    "                idx.add(pos)\n",
    "        elif ch in CLOSE_BRACKETS:\n",
    "            idx.discard(pos)\n",
    "            if pos < n - 1 and (s[pos + 1].isalnum() or s[pos + 1] in OPEN_BRACKETS or s[pos + 1] in {'\"', \"'\"}):\n",
    "                idx.add(pos + 1)\n",
    "\n",
    "    quote_open = True\n",
    "    for pos, ch in enumerate(s):\n",
    "        if ch == '\"':\n",
    "            if quote_open:\n",
    "                idx.discard(pos + 1)\n",
    "                if pos > 0 and (s[pos - 1].isalnum() or s[pos - 1] in CLOSE_BRACKETS):\n",
    "                    idx.add(pos)\n",
    "            else:\n",
    "                idx.discard(pos)\n",
    "                if pos < n - 1 and (s[pos + 1].isalnum() or s[pos + 1] in OPEN_BRACKETS):\n",
    "                    idx.add(pos + 1)\n",
    "            quote_open = not quote_open\n",
    "\n",
    "    single_open = True\n",
    "    for pos, ch in enumerate(s):\n",
    "        if ch == \"'\":\n",
    "            prev_ch = s[pos - 1] if pos > 0 else ''\n",
    "            next_ch = s[pos + 1] if pos < n - 1 else ''\n",
    "            if prev_ch.isalnum() and next_ch.isalnum():\n",
    "                continue\n",
    "            if single_open:\n",
    "                idx.discard(pos + 1)\n",
    "                if pos > 0 and (prev_ch.isalnum() or prev_ch in CLOSE_BRACKETS or prev_ch == '\"'):\n",
    "                    idx.add(pos)\n",
    "            else:\n",
    "                idx.discard(pos)\n",
    "                if pos < n - 1 and (next_ch.isalnum() or next_ch in OPEN_BRACKETS or next_ch == '\"'):\n",
    "                    idx.add(pos + 1)\n",
    "            single_open = not single_open\n",
    "\n",
    "    return idx\n",
    "\n",
    "def fix_punctuation_spacing(orig_text, idx):\n",
    "    s = orig_text\n",
    "    n = len(s)\n",
    "    idx = set(i for i in idx if 0 < i < n)\n",
    "\n",
    "    def is_opening(ch):\n",
    "        return ch in OPEN_BRACKETS or ch in {'\"', \"'\"}\n",
    "\n",
    "    def is_closing(ch):\n",
    "        return ch in CLOSE_BRACKETS or ch in {'\"', \"'\"}\n",
    "\n",
    "    for pos, ch in enumerate(s):\n",
    "        if ch in TRAILING_PUNCT or ch in {DOT, ELLIPSIS}:\n",
    "            idx.discard(pos)\n",
    "\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        if s[i] == DOT:\n",
    "            j = i\n",
    "            while j < n and s[j] == DOT:\n",
    "                idx.discard(j)\n",
    "                j += 1\n",
    "            if j < n and (s[j].isalnum() or is_opening(s[j])):\n",
    "                idx.add(j)\n",
    "            i = j\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    for pos, ch in enumerate(s):\n",
    "        if ch == ELLIPSIS and pos < n - 1 and (s[pos + 1].isalnum() or is_opening(s[pos + 1])):\n",
    "            idx.add(pos + 1)\n",
    "\n",
    "    for pos, ch in enumerate(s):\n",
    "        if ch in TRAILING_PUNCT and pos < n - 1 and (s[pos + 1].isalnum() or is_opening(s[pos + 1])):\n",
    "            idx.add(pos + 1)\n",
    "\n",
    "    for pos in range(n - 1):\n",
    "        ch, nxt = s[pos], s[pos + 1]\n",
    "        if (ch in TRAILING_PUNCT or ch in {DOT, ELLIPSIS} or is_opening(ch) or is_closing(ch)) and \\\n",
    "            (nxt in TRAILING_PUNCT or nxt in {DOT, ELLIPSIS} or is_opening(nxt) or is_closing(nxt)):\n",
    "            idx.discard(pos + 1)\n",
    "\n",
    "    return idx\n",
    "\n",
    "def fix_ru_suffix_clitics(orig_text, idx):\n",
    "    s = orig_text\n",
    "    n = len(s)\n",
    "    idx = set(i for i in idx if 0 < i < n)\n",
    "    for i in range(1, n - 2):\n",
    "        if s[i] == '-' and s[i+1:i+3].lower() == 'то' and s[i-1].isalpha():\n",
    "            idx.discard(i)\n",
    "            idx.discard(i + 1)\n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6891e4d9-aff8-40ea-854c-e3e5e420426f",
   "metadata": {},
   "source": [
    "# Динамическое программирование\n",
    "- Цель: найти разбиение строки, которое максимизирует сумму лог-вероятностей.  \n",
    "- Алгоритм перебирает разрезы до `max_word_len` символов назад и выбирает лучший.  \n",
    "- Из массива `prev` восстанавливаются позиции пробелов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbbdb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_spaces_indices(text: str) -> list:\n",
    "    orig_text = text\n",
    "    text = text.lower()\n",
    "    n = len(text)\n",
    "    max_word_len = 25\n",
    "    \n",
    "    dp = [float(\"-inf\")] * (n+1)\n",
    "    prev = [-1] * (n+1)\n",
    "    dp[0] = 0.0\n",
    "    \n",
    "    for i in range(1, n+1):\n",
    "        for j in range(max(0, i - max_word_len), i):\n",
    "            substr = text[j:i]\n",
    "            if any(1040 <= ord(c) <= 1103 for c in substr):\n",
    "                wf, tf, mf = word_freq_ru, total_freq_ru, max_freq_ru\n",
    "            else:\n",
    "                wf, tf, mf = word_freq_en, total_freq_en, max_freq_en\n",
    "            \n",
    "            logp = get_unigram_logp(substr, wf, tf, mf)\n",
    "            score = dp[j] + logp\n",
    "            if score > dp[i]:\n",
    "                dp[i] = score\n",
    "                prev[i] = j\n",
    "    \n",
    "    indices = []\n",
    "    i = n\n",
    "    while i > 0:\n",
    "        j = prev[i]\n",
    "        if j > 0:\n",
    "            indices.append(j)\n",
    "        i = j\n",
    "    indices.reverse()\n",
    "\n",
    "    idx = fix_bracket_and_quote_spacing(orig_text, indices)\n",
    "    idx = fix_punctuation_spacing(orig_text, idx)\n",
    "    idx = fix_ru_suffix_clitics(orig_text, idx)\n",
    "    return sorted(idx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4572c4c-2af5-41f0-b025-aefdcc944504",
   "metadata": {},
   "source": [
    "# Обработка файлов\n",
    "В строках .csv могут встречаться запятые, поэтому пишем свою реализацию чтения .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47df6306-fb6c-4d26-aeec-ac6f780c772c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_indices(text: str, indices: list) -> str:\n",
    "    out = []\n",
    "    last = 0\n",
    "    for i in indices:\n",
    "        out.append(text[last:i])\n",
    "        out.append(\" \")\n",
    "        last = i\n",
    "    out.append(text[last:])\n",
    "    return \"\".join(out)\n",
    "\n",
    "def process_file(input_file: str, output_file: str):\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "        lines = infile.readlines()\n",
    "        output_data = []\n",
    "        for line in lines[1:]:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            parts = line.split(',', 1)\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "            id_, text_no_spaces = parts\n",
    "            indices = restore_spaces_indices(text_no_spaces)\n",
    "            # restored_text = apply_indices(text_no_spaces, indices)\n",
    "            output_data.append(f'{id_},\"{str(indices)}\"')\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        outfile.write(\"id,predicted_positions\\n\")\n",
    "        for text in output_data:\n",
    "            outfile.write(text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bd5ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"dataset_1937770_3.txt\"\n",
    "output_file = \"output.csv\"\n",
    "\n",
    "if not os.path.exists(input_file):\n",
    "    raise FileNotFoundError(f\"Файл {input_file} не найден\")\n",
    "\n",
    "process_file(input_file, output_file)\n",
    "print(f\"Файл сохранён как {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
