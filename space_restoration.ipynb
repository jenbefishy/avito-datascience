{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28f21de4",
   "metadata": {},
   "source": [
    "# Алгоритм (кратко, одной ячейкой)\n",
    "\n",
    "**Цель:** восстановить позиции пропусков (пробелов) в слепленной строке.\n",
    "\n",
    "**Идея:** динамическое программирование по униграммной языковой модели.\n",
    "1. Загружаем частоты слов RU/EN. `logP(w) = log(freq/total)`.\n",
    "2. Эвристики: спец-термины (бренды/модели), числа, пунктуация, дефисы/тире, версии `x.y`. Язык подстроки — по алфавиту.\n",
    "3. DP: для каждой позиции `i` ищем лучший разрез `j<i` (не дальше `max_word_len`), максимизируя сумму лог-вероятностей.\n",
    "4. Из `prev` восстанавливаем границы и возвращаем **индексы** пробелов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3aee4a93-6e7e-4929-83db-a056f3cd79a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f8d681b-b70b-47e3-8a2e-eb51cca2a24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Символы пунктуации\n",
    "punct = string.punctuation + '-…«»'\n",
    "attaching_punct = set(punct) - {'-'}\n",
    "\n",
    "# Список \"особых\" терминов\n",
    "special_terms = [\n",
    "    'USSR', 'ЕГЭ', 'iPhone', 'HUAWEI', 'Huawei', 'Playstation', 'PLAYSTATION', 'Philips', 'HP', 'Samsung', \n",
    "    'Fender', 'Xbox One', 'Xiaomi', 'Indesit', 'Merida', 'LG', 'Yamaha', 'Ikea', \n",
    "    'Asus', 'Dyson', 'Stels', 'Oppo', 'Honor', 'Bosch', 'Adidas', 'TP-Link', 'JBL', \n",
    "    'Levi’s', 'Canon', 'Pixel', 'MSI', 'Gibson', 'Atlant', 'TCL', 'Lenovo', 'Sharp', \n",
    "    'JVC', 'Acer', 'Dell', 'ROG', 'Toshiba', 'Mac', 'OLED', 'LG OLED', 'A54', 'XR', 'RTX', \n",
    "    'Avito', 'Яндекс Еда', 'Сбербанк', 'шкаф-купе', 'стиралок', 'вайфай', '5.1',\n",
    "    'Samsung A54', 'вайфай 6'\n",
    "]\n",
    "\n",
    "# Создание mapping для специальных терминов (concatenated версии)\n",
    "special_mapping = {}\n",
    "for t in special_terms:\n",
    "    low_t = t.lower()\n",
    "    concat = re.sub(r'[\\s-]', '', low_t)\n",
    "    special_mapping[concat] = t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fa04c1-2261-4928-a228-033874f030d6",
   "metadata": {},
   "source": [
    "## Загрузка словаря частот"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02726fb6-aafc-4d09-90bc-37e70f4663c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружаю словари...\n"
     ]
    }
   ],
   "source": [
    "def get_word_freq(lang='ru'):\n",
    "    if lang == 'ru':\n",
    "        url = \"https://raw.githubusercontent.com/hermitdave/FrequencyWords/master/content/2018/ru/ru_full.txt\"\n",
    "    elif lang == 'en':\n",
    "        url = \"https://raw.githubusercontent.com/hermitdave/FrequencyWords/master/content/2018/en/en_full.txt\"\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported language\")\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        lines = response.text.splitlines()\n",
    "        word_freq = {}\n",
    "        for line in lines:\n",
    "            parts = line.split(' ')\n",
    "            if len(parts) == 2:\n",
    "                word, freq = parts\n",
    "                word_freq[word.lower()] = int(freq)\n",
    "        return word_freq\n",
    "    else:\n",
    "        raise Exception(f\"Failed to download {lang} frequency list\")\n",
    "\n",
    "print(\"Загружаю словари...\")\n",
    "word_freq_ru = get_word_freq('ru')\n",
    "total_freq_ru = sum(word_freq_ru.values())\n",
    "max_freq_ru = max(word_freq_ru.values()) if word_freq_ru else 1\n",
    "\n",
    "word_freq_en = get_word_freq('en')\n",
    "total_freq_en = sum(word_freq_en.values())\n",
    "max_freq_en = max(word_freq_en.values()) if word_freq_en else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f408d00-43c0-48d1-b121-2de310ea1d44",
   "metadata": {},
   "source": [
    "## Вероятность отдельного слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72dd2cb7-654f-4eb8-98af-da5de7e632fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unigram_logp(word, word_freq, total_freq, max_freq):\n",
    "    alpha = 0.05\n",
    "    epsilon = 1e-9\n",
    "    if word in special_mapping:\n",
    "        return math.log(max_freq * 1e10 / total_freq)\n",
    "    if word in word_freq:\n",
    "        return math.log(word_freq[word] / total_freq)\n",
    "    elif word.isdigit():\n",
    "        return math.log(max_freq * 10 / total_freq)\n",
    "    elif len(word) > 0 and all(c in punct for c in word):\n",
    "        return math.log(max_freq * 50 / total_freq)\n",
    "    \n",
    "    # Обработка составных слов с дефисами или тире\n",
    "    for sep in ['-']:\n",
    "        if sep in word:\n",
    "            parts = word.split(sep)\n",
    "            parts = [p for p in parts if p]\n",
    "            if len(parts) > 1 and all(p and (p in word_freq or len(p) > 1 or p.isdigit()) for p in parts):\n",
    "                sub_logps = [get_unigram_logp(p, word_freq, total_freq, max_freq) for p in parts]\n",
    "                sep_logp = math.log(max_freq * 50 / total_freq)  # как для пунктуации\n",
    "                logp = sum(sub_logps) + (len(parts) - 1) * sep_logp + math.log(1.01)  # небольшой бонус для объединения\n",
    "                return logp\n",
    "    \n",
    "    # Обработка чисел с точкой (версии, типа 5.1)\n",
    "    if '.' in word:\n",
    "        parts = word.split('.')\n",
    "        parts = [p for p in parts if p]\n",
    "        if len(parts) > 1 and all(p.isdigit() for p in parts):\n",
    "            sub_logps = [get_unigram_logp(p, word_freq, total_freq, max_freq) for p in parts]\n",
    "            sep_logp = math.log(max_freq * 50 / total_freq)\n",
    "            logp = sum(sub_logps) + (len(parts) - 1) * sep_logp + math.log(1.01)\n",
    "            return logp\n",
    "    \n",
    "    return math.log(epsilon) + (len(word) - 1) * math.log(alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbbc578-71dc-4d00-b56c-3c3574ff23f5",
   "metadata": {},
   "source": [
    "## Восстановление пробелов (DP с униграммами) и возврат индексов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9b125e6-7e21-4036-9eb0-8e0860257881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_spaces_indices(text: str) -> list:\n",
    "    orig_text = text\n",
    "    text = text.lower()\n",
    "    n = len(text)\n",
    "    max_word_len = 25\n",
    "    \n",
    "    dp = [float(\"-inf\")] * (n+1)\n",
    "    prev = [-1] * (n+1)\n",
    "    prev_word = [\"\"] * (n+1)\n",
    "    dp[0] = 0.0\n",
    "    \n",
    "    for i in range(1, n+1):\n",
    "        for j in range(max(0, i-max_word_len), i):\n",
    "            substr = text[j:i]\n",
    "            if any(1040 <= ord(c) <= 1103 for c in substr):\n",
    "                wf, tf, mf = word_freq_ru, total_freq_ru, max_freq_ru\n",
    "            else:\n",
    "                wf, tf, mf = word_freq_en, total_freq_en, max_freq_en\n",
    "            \n",
    "            word = substr\n",
    "            logp = get_unigram_logp(word, wf, tf, mf)\n",
    "            \n",
    "            score = dp[j] + logp\n",
    "            if score > dp[i]:\n",
    "                dp[i] = score\n",
    "                prev[i] = j\n",
    "                prev_word[i] = word\n",
    "    \n",
    "\n",
    "    indices = []\n",
    "    i = n\n",
    "    while i > 0:\n",
    "        j = prev[i]\n",
    "        if j > 0:  # Добавляем индекс только если это не начало строки\n",
    "            indices.append(j)\n",
    "        i = j\n",
    "    indices.reverse() \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76c8a95f-9109-457c-8e5e-dd1883375d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обработка файла\n",
    "def process_file(input_file: str, output_file: str):\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "        lines = infile.readlines()\n",
    "        output_data = []\n",
    "        for line in lines[1:]:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            parts = line.split(',', 1)\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "            id_, text_no_spaces = parts\n",
    "            indices = restore_spaces_indices(text_no_spaces)\n",
    "            output_data.append(f'{id_},\"{str(indices)}\"')\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        outfile.write(\"id,predicted_positions\\n\")\n",
    "        for text in output_data:\n",
    "            outfile.write(text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c279cec7-d9e9-4468-8ac4-7ffca31c3b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Файл сохранён как output.csv\n"
     ]
    }
   ],
   "source": [
    "input_file = \"dataset_1937770_3.txt\"\n",
    "output_file = \"output.csv\"\n",
    "\n",
    "if not os.path.exists(input_file):\n",
    "    raise FileNotFoundError(f\"Файл {input_file} не найден\")\n",
    "\n",
    "process_file(input_file, output_file)\n",
    "print(f\"Файл сохранён как {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
